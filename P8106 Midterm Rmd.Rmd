---
title: "P8106 Data Science Homework 2"
author: "Emil Hafeez, eh2928"
geometry: "left=2.57cm,right=2.57cm,top=2.57cm,bottom=2.57cm"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

```{r libraries, include = F}
library(caret) 
library(klaR)
library(ggplot2)
library(patchwork)
library(pROC)
library(corrplot)
library(tidyverse)
```

```{r opts chunk, include = F}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Parameters for Report

Your report should have the following sections (you can add other sections if you want) and should be no more than 3 pages, excluding figures and tables. The total number of figures and tables should not exceed 6.

# Section 1: Introduction

## Describe your data set. Provide proper motivation for your work.

Load the dataset and describe the outcome and predictors. Provide summary statistics; examine its structure (and unit of analysis), as well as its cleanliness. Go into more detail about the outcome to provide motivation, and consider a citation.

This dataset contains 5110 observations, with de-identified individual patients as the unit of analysis. Each patient is characterized by their id, and a set of numerical and factor variables detailing basic demographic and biomedical information, like their gender, age, marriage status, and glucose level. The outcome of interest is a binary indicator of stroke. 

While limited information is provided by the data source, this dataset is thought to consider xxx or yyy type of stroke as stroke outcome.
major burden of disease.
preventative would be good.
WRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITE. CITE. 

```{r start clean, include = F, message = F}
stroke_df = read_csv("./data/healthcare-dataset-stroke-data.csv", na = "N/A")

stroke_df = 
  stroke_df %>% 
  mutate(
    id = as.factor(id),
    gender = as.factor(gender),
    hypertension = as.factor(hypertension),
    heart_disease = as.factor(heart_disease),
    ever_married = as.factor(ever_married),
    work_type = as.factor(work_type),
    Residence_type = as.factor(Residence_type),
    smoking_status = as.factor(smoking_status),
    stroke = as.factor(stroke)
  ) %>% 
  rename(residence_type = Residence_type) %>% 
  dplyr::select(id, stroke, gender, age, avg_glucose_level, bmi, everything()) %>% 
    mutate(
    stroke = factor(stroke, labels = make.names(levels(stroke))),
    hypertension = factor(hypertension, labels = make.names(levels(hypertension))),
    heart_disease = factor(heart_disease, labels = make.names(levels(heart_disease)))
  ) %>% 
  dplyr::select(-id) %>% 
  filter(!gender %in% c("Other")) %>% 
  mutate(
    gender = factor(gender)
  )

rnoaa::vis_miss(stroke_df)

stroke_df_no_na = na.omit(stroke_df)

stroke_df_no_na = 
  stroke_df_no_na %>% 
  dplyr::select(stroke, age, avg_glucose_level, bmi, everything())
```

```{r matrix inputs,include=F}
# matrix of predictors
x = model.matrix(stroke ~ ., stroke_df_no_na)[,-1]
#vector of response
y = stroke_df_no_na$stroke
```


```{r include = F}
summary(stroke_df)
```
## What questions are you trying to answer?

This is a public health example; trying to most effectively predict stroke (as a binary classification). Simple dataset. Relevant predictors, diagnostic criteria. Thus, this is a classification question.

How to allocate resource and to promote special precautions is hard.
decision support system.
thus, evaluate the feasibility of several predictive methods and determine if sufficient accuracy is available from a simple dataset like this.
ideally, also get insight into which factors may be most relevant.

## How did you prepare and clean the data?

The data required minimal steps in order to prepare and clean it. Essentially, after importing the data, variables were transmuted into more appropriate data types for analysis (for example, character variables into factor variables). Then, the whole dataset was analyzed for missing data, and tidyverse "data tidying" best practices were implemented to rename and reorder variables. The BMI variable was missing 201 observations, which were assumed to be due to a missing-at-random and thus omitted. The caret package was used to create testing and training subsets, and some objects (such as a predictor matrix) were created for modeling purposes, as well as some centering and scaling for certain analytic methods.


# Section 2: Exploratory analysis/visualization

## Is there any interesting structure present in the data?

WRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITE
WRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITEWRITE
There are many categorical variables, as well as a few nominal and ordinal variables. 

There is only 1 "Other" in gender.


//////// create plots for bmi vs glucose, for age histogram, glucose dist, bmi dist, stroke and gender
//////// plot gender with age, glucose, bmi, gender vs other categoricals
////////correlation plot

```{r visualization, include = F}
#histograms
p2 = 
  ggplot(data = stroke_df_no_na) +
  geom_histogram(aes(x = age), fill = "blue", alpha = 0.75) 
p2

p3 = 
  ggplot(data = stroke_df_no_na) +
  geom_histogram(aes(x = avg_glucose_level), fill = "blue", alpha = 0.75) 
p3

p4 = 
  ggplot(data = stroke_df_no_na) +
  geom_histogram(aes(x = bmi), fill = "blue", alpha = 0.75) 
p4

#boxplots
p5 = 
  ggplot(data = stroke_df_no_na) +
  geom_boxplot(aes(x = age, fill = gender), alpha = .75) 
p5

p6 = 
  ggplot(data = stroke_df_no_na) +
  geom_boxplot(aes(x = age, fill = stroke), alpha = .75) +
  scale_fill_discrete(
                      labels=c("No Stroke", "Stroke"))
p6

p7 = 
  ggplot(data = stroke_df_no_na) +
  geom_boxplot(aes(x = avg_glucose_level, fill = gender), alpha = .75) +
  scale_fill_discrete(
                      labels=c("No Stroke", "Stroke"))
p7

#scatters
p1 = 
  ggplot(data = stroke_df_no_na) +
  geom_point(aes(x = avg_glucose_level, y = bmi, color = age), alpha = 0.75)
p1

(p2 + p3 + p4) / (p5 + p6 +p7) / (p1)
```

```{r}
corrplot(cor(x), method = "square", type = "upper")
```
## What were your findings?

Describe the summary statistics and talk about the potential relevance; anything else?

## Here you can use any techniques as long as they are adequately explained. If you cannot find anything interesting, then describe what you tried and show that there isnâ€™t much visible structure. Data science is NOT manipulating the data in some way until you get an answer.
 
# Section 3: Models (Biggest section!!)

```{r training selection object, include = F}
set.seed(1107)
trRows <- createDataPartition(stroke_df_no_na$stroke,
                              p = .75,
                              list = F)
```

```{r modeling 1, include = F}
# Using caret
set.seed(1107)
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
```

```{r modeling approaches and graphs, include = F, cache = T}
#ranger rf
set.seed(1107)
ctrl <- trainControl(method = "cv",
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary)
#rf
rf.grid <- expand.grid(mtry = 1:11,
                       splitrule = "gini",
                       min.node.size = seq(from = 1, to = 100, by = 5))
set.seed(1107)
rf.fit <- train(stroke ~ . , 
                stroke_df_no_na, 
                subset = trRows,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)
#coninf tree
set.seed(1107)
ctree.fit <- train(stroke ~ . , 
                   stroke_df_no_na, 
                   subset = trRows,
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-2, -1, length = 50))),
                   metric = "ROC",
                   trControl = ctrl)
#gbm
set.seed(1107)
gbm.grid <- expand.grid(n.trees = c(2000,3000,4000),
                        interaction.depth = 1:4,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = c(1,10))
gbm.fit <- train(stroke ~ . , 
                 stroke_df_no_na, 
                 subset = trRows,
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 trControl = ctrl,
                 verbose = FALSE)
#gbm ada
gbmA.grid <- expand.grid(n.trees = c(2000,3000,4000),
                         interaction.depth = 1:6,
                         shrinkage = c(0.001,0.003,0.005),
                         n.minobsinnode = 1)
set.seed(1107)
gbmA.fit <- train(stroke ~ . , 
                  stroke_df_no_na, 
                  subset = trRows, 
                  tuneGrid = gbmA.grid,
                  trControl = ctrl,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)

ggplot(rf.fit, highlight = TRUE)
ggplot(ctree.fit, highlight = TRUE)
ggplot(gbm.fit, highlight = TRUE)
ggplot(gbmA.fit, highlight = TRUE)
```

## What predictor variables did you include?

All variables from the dataset are included in modeling other than the arbitrary numeric identifier. The dataset is not high dimensional, and no variable selection procedures prior to modeling were implemented. Rather than parsing the number of predictor variables, this analysis was primarily limited by the large quantity of categorical variables in the predictor set, since this limited the algorithms which could be applied to the training data.

## What technique did you use? What assumptions, if any, are being made by using this technique?

This application focuses on tree-based methods and ensemble methods, given the predictor space. Specifically, techniques used include random forests, conditional inference trees, and gradient boosting (and adaboost). These methods are utilized because they have few distributional assumptions and are adaptable to our categorical predictors. Random forests makes no formal distributional assumptions, and are non-parametric. Conditional inference trees are closely related to decision trees, but uses a significance test to select inputs rather than maximizing an information measure (like the Gini coefficient used in our RF). Gradient boosting and AdaBoost are used to address the limits of bagging, using weak classifiers and weighting to find best fits and combine classifiers, assuming that observations are independent, and sometimes assumptions about the interaction depth (though this is tuned here).

## If there were tuning parameters, how did you pick their values?

Tuning parameters include the minimum node size in the random forest algorithm and the minimum criterion in the conditional inference trees; the gradient boosting algorithm and the Adaptive Boosting version tunes the number of trees, interaction dept, shrinkage, and the minimum number of observations in each node. These were tuned using cross-validation, using a wider range and coarser search pattern; after locating approximate ranges for the selected parameters, I then iterated the search pattern within a narrower ranger and with more granulars steps.

## Discuss the training/test performance if you have a test data set.

Comment on the below.

```{r modeling 3 comparison, include = F}
res <- resamples(list(
                      knn = knn.fit,
                      rf = rf.fit,
                      ctree = rf.fit,
                      gbm = rf.fit,
                      gbma = rf.fit
                      )
                 )
summary(res)

bwplot(res, metric = "ROC")
```

```{r modeling 4 test performance, include = F}
rf.pred <- predict(rf.fit, newdata = stroke_df_no_na[-trRows,], type = "prob")[,1]
ctree.pred <- predict(ctree.fit, newdata = stroke_df_no_na[-trRows,], type = "prob")[,1]
gbm.pred <- predict(gbm.fit, newdata = stroke_df_no_na[-trRows,], type = "prob")[,1]
gbmA.pred <- predict(gbmA.fit, newdata = stroke_df_no_na[-trRows,], type = "prob")[,1]

roc.rf <- roc(stroke_df_no_na$stroke[-trRows], rf.pred)
roc.ctree <- roc(dat$diabetes[-trRows], ctree.pred)
roc.gbm <- roc(dat$diabetes[-trRows], gbm.pred)
roc.gbmA <- roc(dat$diabetes[-trRows], gbmA.pred)

auc <- c(roc.rf$auc[1], roc.ctree$auc[1], roc.gbm$auc[1],
         roc.gbmA$auc[1])

#plot
plot(roc.rf, legacy.axes = TRUE)
plot(roc.ctree, col = 2, add = TRUE)
plot(roc.gbm, col = 3, add = TRUE)
plot(roc.gbmA, col = 4, add = TRUE)
modelNames <- c("rf","ctree","gbm","gbmA")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:4, lwd = 2)
```

## Which variables play important roles in predicting the response?

Compute this and edit it.
```{r}
## add variable importance 
set.seed(1)
rf2.final.per <- ranger(Salary ~ . , 
                        Hitters[trRows,],
                        mtry = rf.fit$bestTune[[1]], 
                        splitrule = "variance",
                        min.node.size = rf.fit$bestTune[[3]],
                        importance = "permutation",
                        scale.permutation.importance = TRUE) 

barplot(sort(ranger::importance(rf2.final.per), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(19))

set.seed(1)
rf2.final.imp <- ranger(Salary ~ . , 
                        Hitters[trRows,],
                        mtry = rf.fit$bestTune[[1]], 
                        splitrule = "variance",
                        min.node.size = rf.fit$bestTune[[3]],
                        importance = "impurity") 

barplot(sort(ranger::importance(rf2.final.imp), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(19))

#Variable importance from boosting can be obtained using the `summary()` function.
summary(gbm.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
summary(gbmA.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

## What are the limitations of the models you used (if there are any)? Are the models flexible enough to capture the underlying truth?

Discuss limitations of selected models via their lecture notes.

# Section 4: Conclusions

## What were your findings? Are they what you expect? What insights into the data can you make?

#Section 666: OPEN QUESTIONS
How to deal with this imbalanced data? Consider adding to data cleaning steps.
What is the influence of so many predictors here?
add viridis....
WHICH OF THE EDA PLOTS TO KEEP IN THE REPORT?
add corrplot to eda
what assumptions does the gradient boosting make
do i need to be inputting a matrix or selecting the outcome?

what about:
```{r modeling 2, include = F}
#from lecture 10
#qda
#knn
knn.fit <- train(stroke_df_no_na[,2:11],
                   stroke_df_no_na,
                   subset = trRows,
                   method = "knn",
                   preProcess = c("center","scale"),
                   metric = "ROC",
                   tuneGrid = data.frame(k = seq(1,200,by=10)),
                   trControl = ctrl)

ggplot(knn.fit, highlight = TRUE)

```
Or, the SVM?
