---
title: "P8106 Data Science Homework 2"
author: "Emil Hafeez, eh2928"
geometry: "left=2.57cm,right=2.57cm,top=2.57cm,bottom=2.57cm"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

```{r libraries, include = F}
library(caret) 
library(splines)
library(mgcv)
library(pdp)
library(earth)
library(tidyverse)
library(ggplot2)
```

```{r opts chunk, include = F}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
```

# Parameters for Report

Your report should have the following sections (you can add other sections if you want) and should be no more than 3 pages, excluding figures and tables. The total number of figures and tables should not exceed 6.

EMIL CONSIDERING USING THE ENDFLOAT PACKAGE IN LATEX TO GET ALL THE FIGURES INTO THE APPENDIX
https://tex.stackexchange.com/questions/164140/how-to-tell-latex-to-place-all-figures-at-the-end-of-pdf-file

# Section 1: Introduction

## Describe your data set. Provide proper motivation for your work.

Load the dataset and describe the outcome and predictors. Provide summary statistics; examine its structure (and unit of analysis), as well as its cleanliness. Go into more detail about the outcome to provide motivation, and consider a citation.

This dataset contains 5110 observations, with de-identified individual patients as the unit of analysis. Each patient is characterized by their id, and a set of numerical and factor variables detailing basic demographic and biomedical information, like their gender, age, marriage status, and glucose level. The outcome of interest is a binary indicator of stroke. 

While limited information is provided by the data source, this dataset is thought to consider xxx or yyy type of stroke as stroke outcome.
major burden of disease.
preventative would be good.
COMEBACKTOTHISCOMEBACKTOTHISCOMEBACKTOTHISCOMEBACKTOTHISCOMEBACKTOTHISCOMEBACKTOTHIS.CITE.

```{r start, include = F, message = F}
stroke_df = read_csv("./data/healthcare-dataset-stroke-data.csv", na = "N/A")

stroke_df = 
  stroke_df %>% 
  mutate(
    id = as.factor(id),
    gender = as.factor(gender),
    hypertension = as.factor(hypertension),
    heart_disease = as.factor(heart_disease),
    ever_married = as.factor(ever_married),
    work_type = as.factor(work_type),
    Residence_type = as.factor(Residence_type),
    smoking_status = as.factor(smoking_status),
    stroke = as.factor(stroke)
  ) %>% 
  rename(residence_type = Residence_type) %>% 
  select(id, stroke, gender, age, everything())

rnoaa::vis_miss(stroke_df)
```

```{r include = F}
summary(stroke_df)
dim(stroke_df[duplicated(stroke_df$id),])[1]
```

```{r training selection object, include = F}
set.seed(1107)
trRows <- createDataPartition(stroke_df$stroke,
                              p = .75,
                              list = F)
```
## What questions are you trying to answer?

This is a public health example; trying to most effectively predict stroke (as a binary classification). Simple dataset. Relevant predictors, diagnostic criteria. Thus, this is a classification question.

How to allocate resource and to promote special precautions is hard.
decision support system.
thus, evaluate the feasibility of several predictive methods and determine if sufficient accuracy is available from a simple dataset like this.
ideally, also get insight into which factors may be most relevant.


## How did you prepare and clean the data?

The data required minimal steps in order to prepare and clean it. Essentially, after importing the data, variables were transmuted into more appropriate data types for analysis (for example, character variables into factor variables). Then, the whole dataset was analyzed for missing data, and tidyverse "data tidying" best practices were implemented to rename and reorder variables. The caret package was used to create testing and training subsets. Straightforward mise en place. 

ADD MATRICES CREATION FOR ANALYSIS AND PLOTS, STANDARDIZING FOR CARET.
WILL I OMIT THE MISSING BMI?


# Section 2: Exploratory analysis/visualization

## Is there any interesting structure present in the data?

Do the plots; the one that creates pairwise scatter plots, and maybe the one from the knitted lecture 10 rmd. Comment on findings.

RESUMEHERERESUMEHERERESUMEHERERESUMEHERERESUMEHERERESUMEHERERESUMEHERERESUMEHERERESUMEHERERESUMEHERERESUMEHERERESUMEHERERESUMEHERERESUMEHERERESUMEHERERESUMEHERERESUMEHERERESUMEHERERESUMEHERE
MUSTEDITMUSTEDITMUSTEDITMUSTEDITMUSTEDITMUSTEDITMUSTEDITMUSTEDITMUSTEDIT
```{r form matrix, include = F}
#remove rows with missing bmi, assuming MAR
stroke_df_no_na = na.omit(stroke_df)

# matrix of predictors
x = model.matrix(stroke ~ ., stroke_df_no_na)[,3:12]
#vector of response
y = stroke_df_no_na$stroke
```

Here, we create the matrix for visualization, and plot the data against the outcome.
```{r visualization, include = F}
#from lecture 10
partimat(diabetes ~ glucose + age + mass + pedigree, 
         data = dat, subset = rowTrain, method = "lda")

partimat(diabetes ~ glucose + age + mass + pedigree, 
         data = dat, subset = rowTrain, method = "qda")

# partimat(diabetes ~ glucose + age + mass + pedigree, 
#          data = dat, subset = rowTrain, method = "naiveBayes")


stroke_df_vis = 
  stroke_df_no_na %>% 
  select(id, stroke, age, avg_glucose_level, bmi, everything())

# matrix of predictors
x_vis = model.matrix(stroke ~ ., stroke_df_vis)[,4:5]
#vector of response as-is, using 'y'

featurePlot(x = x_vis, 
            y = stroke_df$age,
            labels = c("Predictor Value","Out of State Tuition"),
            plot = "scatter", pch = ".",
            auto.key = list(columns = 2))


stackbar_examp <- ggplot(data = stroke_df) + 
  geom_mosaic(aes(x=product(do_you_recline, rude_to_recline), fill = do_you_recline),
              divider=c("vspine", "hbar")) +   
  labs(x="Is it rude to recline?", y = "Do you recline?", title = "Stacked Bar Chart")

bwplot(data2$Volume)

```


## What were your findings?

Describe the summary statistics and talk about the potential relevance; anything else?

## Here you can use any techniques as long as they are adequately explained. If you cannot find anything interesting, then describe what you tried and show that there isnâ€™t much visible structure. Data science is NOT manipulating the data in some way until you get an answer.
 
# Section 3: Models (Biggest section!!)

```{r modeling 1, include = F}
# Using caret
#from lecture 9
#logistic regression
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
set.seed(1)
model.glm <- train(x = dat[rowTrain,1:8],
                   y = dat$diabetes[rowTrain],
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl)

#penalized logistic regression
model.glmn <- train(x = dat[rowTrain,1:8],
                    y = dat$diabetes[rowTrain],
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl)

plot(model.glmn, xTrans = function(x) log(x))   

#mars
model.mars <- train(x = dat[rowTrain,1:8],
                    y = dat$diabetes[rowTrain],
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:3, 
                                           nprune = 2:15),
                    metric = "ROC",
                    trControl = ctrl)

plot(model.mars)

coef(model.mars$finalModel) 

pdp::partial(model.mars, pred.var = c("age"), grid.resolution = 200) %>% autoplot()

vip(model.mars$finalModel)

```

```{r modeling 2, include = F}
#from lecture 10
#qda
model.qda <- train(x = dat[rowTrain,1:8],
                   y = dat$diabetes[rowTrain],
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl)

#naive bayes
nbGrid <- expand.grid(usekernel = c(FALSE,TRUE),
                      fL = 1, 
                      adjust = seq(.2, 3, by = .2))

model.nb <- train(x = dat[rowTrain,1:8],
                  y = dat$diabetes[rowTrain],
                  method = "nb",
                  tuneGrid = nbGrid,
                  metric = "ROC",
                  trControl = ctrl)

plot(model.nb)
```

```{r modeling, include=F}
#from lecture 12
ctrl <- trainControl(method = "cv",
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary)

rf.grid <- expand.grid(mtry = 1:8,
                       splitrule = "gini",
                       min.node.size = seq(from = 2, to = 10, by = 2))
set.seed(1)
rf.fit <- train(diabetes ~ . , 
                dat, 
                subset = rowTrain,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)

ggplot(rf.fit, highlight = TRUE)

rf.pred <- predict(rf.fit, newdata = dat[-rowTrain,], type = "prob")[,1]
```

```{r modeling 3 comparison, include = F}
res <- resamples(list(GLM = model.glm,
GLMNET = model.glmn,
GAM = model.gam,
MARS = model.mars,
KNN = model.knn))

summary(res)

bwplot(res, metric = "ROC")
```

```{r modeling 4 test performance, include = F}
#from lecture 9
glm.pred <- predict(model.glm, newdata = dat[-rowTrain,], type = "prob")[,2]
glmn.pred <- predict(model.glmn, newdata = dat[-rowTrain,], type = "prob")[,2]
knn.pred <- predict(model.knn, newdata = dat[-rowTrain,], type = "prob")[,2]
gam.pred <- predict(model.gam, newdata = dat[-rowTrain,], type = "prob")[,2]
mars.pred <- predict(model.mars, newdata = dat[-rowTrain,], type = "prob")[,2]
#from lecture 10
lda.pred <- predict(model.lda, newdata = dat[-rowTrain,], type = "prob")[,2]
nb.pred <- predict(model.nb, newdata = dat[-rowTrain,], type = "prob")[,2]
qda.pred <- predict(model.qda, newdata = dat[-rowTrain,], type = "prob")[,2]
#combined
roc.glm <- roc(dat$diabetes[-rowTrain], glm.pred)
roc.glmn <- roc(dat$diabetes[-rowTrain], glmn.pred)
roc.knn <- roc(dat$diabetes[-rowTrain], knn.pred)
roc.gam <- roc(dat$diabetes[-rowTrain], gam.pred)
roc.mars <- roc(dat$diabetes[-rowTrain], mars.pred)
roc.lda <- roc(dat$diabetes[-rowTrain], lda.pred)
roc.nb <- roc(dat$diabetes[-rowTrain], nb.pred)
roc.qda <- roc(dat$diabetes[-rowTrain], qda.pred)
#combined
auc <- c(roc.glm$auc[1], roc.glmn$auc[1], roc.knn$auc[1],
         roc.gam$auc[1], roc.mars$auc[1], roc.lda$auc[1], roc.qda$auc[1], roc.nb$auc[1])
#combined
plot(roc.glm, legacy.axes = TRUE)
plot(roc.glmn, col = 2, add = TRUE)
plot(roc.knn, col = 3, add = TRUE)
plot(roc.gam, col = 4, add = TRUE)
plot(roc.mars, col = 5, add = TRUE)
plot(roc.lda, legacy.axes = TRUE)
plot(roc.qda, col = 2, add = TRUE)
plot(roc.nb, col = 3, add = TRUE)
modelNames <- c("glm","glmn","knn","gam","mars")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:5, lwd = 2)
modelNames <- c("lda","qda","nb")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:3, lwd = 2)
```


## What predictor variables did you include?

All variables from the dataset are included in modeling other than the arbitrary numeric identifier. The dataset is not high dimensional, and no variable selection procedures prior to modeling were implemented. 

## What technique did you use? What assumptions, if any, are being made by using this technique?

Logistic, Penalized logistic, GAM, MARS, LDA, QDA, naive bayes' available.

Looks like logistic regression, linear discriminant analysis (useful when classes are well-separated), and maybe use QDA and maybe KNN. Decide.

## If there were tuning parameters, how did you pick their values?

CV; describe function(s) using the tuning parameter grid.

## Discuss the training/test performance if you have a test data set.

Make a box plot and discuss the winner in terms of mean RMSE or diagnostic criteria/ROC/AUC. 

## Which variables play important roles in predicting the response?

## What are the limitations of the models you used (if there are any)? Are the models flexible enough to capture the underlying truth?
Discuss limitations of selected models via ther lecture notes.

# Section 4: Conclusions

## What were your findings? Are they what you expect? What insights into the data can you make?


#Section 666: OPEN QUESTIONS
How to deal with this imbalanced data? COnsider adding to data cleaning steps.
What is the influence of so many predictors here?
WILL I OMIT THE MISSING BMI?